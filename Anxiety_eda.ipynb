{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f432da9b-9613-4d08-9527-90623176ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "833c997d-7da6-40b3-83e8-c4bff8152862",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/reddit_posts_comments_Anxiety.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "402ada55-f408-449e-96c0-00f077e6c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # Show all column content\n",
    "pd.set_option('display.max_columns', None)  # Show all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6b8e243-665c-42b6-ba51-35ff538acc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = df['Post Title']\n",
    "df_body = df['Post Body']\n",
    "df_comment = df['Comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a08f921-7cc6-4443-8e5b-a4187ea21f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_post = pd.DataFrame(df_post)\n",
    "df_post.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "859931f5-e998-4b2b-86ae-84d20e9e7664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_body = pd.DataFrame(df_body)\n",
    "df_body.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ffeb31a-09da-440a-920d-d335823ddbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_comment = pd.DataFrame(df_comment)\n",
    "df_comment.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ed96988-fc45-4802-b4c4-f4cdbb2512ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d53fc614-fda2-4304-8f39-b796e4a38383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(lambda m: m.strip().lower())\n",
    "df_body['Post Body'] = df_body['Post Body'].astype(str).apply(lambda m: m.strip().lower())\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(lambda m: m.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1130d60e-e623-4815-86f0-74cf1d40bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean and retain only words (strings)\n",
    "def clean_text(x):\n",
    "    if isinstance(x, str):  # Ensure it's a string\n",
    "        # Use regex to extract words, join them with spaces, and convert to lowercase\n",
    "        return \" \".join(re.findall(r'\\b\\w+', x))\n",
    "    return None  # Discard non-strings\n",
    "\n",
    "# Apply the cleaning function to the DataFrame columns\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(clean_text)\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(clean_text)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05bee329-0ace-4e1f-b791-a198db32fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(lambda text: re.sub(r'\\s+', ' ',re.sub(r'_', ' ', re.sub(r'\\d+', '', text))).strip())\n",
    "\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(lambda text: re.sub(r'\\s+', ' ',re.sub(r'_', ' ', re.sub(r'\\d+', '', text))).strip())\n",
    "\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(lambda text: re.sub(r'\\s+', ' ',re.sub(r'_', ' ', re.sub(r'\\d+', '', text))).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d5023aa-4976-4d83-ac31-0d147e9b732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d892583-64a2-4d28-b4aa-b0ab0119873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eddac6ff-ac88-4857-80e4-74e8b84789f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(tokenizer.tokenize)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(tokenizer.tokenize)\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edc02c8e-d9c9-4ecc-b868-50d6facbdca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d513772c-a098-49eb-a384-4c90683e2acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\navne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c2186be-7ec2-4857-976c-a422414ff6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44023891-733f-47f3-a237-1c4374f1d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(message):\n",
    "    global stop_words\n",
    "    result = []\n",
    "    for word in str(message).split():  # Ensure the input is a string\n",
    "        if word.lower().strip() not in stop_words:\n",
    "            result.append(word)\n",
    "    return \" \".join(result)\n",
    "\n",
    "# Apply the function to each entry in the Post Body column\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)  # Convert lists to strings\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(remove_stopwords)\n",
    "\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)  # Convert lists to strings\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(remove_stopwords)\n",
    "\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)  # Convert lists to strings\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b61c77ad-282b-48d5-a8c1-b7825f80513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(tokenizer.tokenize)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(tokenizer.tokenize)\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae1def95-5402-4185-bf59-b1ada059a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment = df_comment[df_comment[\"Comment\"].apply(lambda x: x != [\"removed\"])]\n",
    "df_comment = df_comment[df_comment[\"Comment\"].apply(lambda x: x != [\"deleted\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "958309e0-f25a-46ab-90fa-dadba91ca3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_comment.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6216163a-508c-4def-b550-20e0808bbb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = df_post.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "476a7da5-f999-4cb9-92b1-d87be9421351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_post = df_post.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3f0c140-f6e9-4cc7-830d-df299024ac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"\n",
    "    Map POS tag to first character lemmatize() accepts.\n",
    "    Only adjectives (a) and verbs (v) are considered.\n",
    "    \"\"\"\n",
    "    tag = wordnet.synsets(word)\n",
    "    if tag:\n",
    "        pos = tag[0].pos()\n",
    "        if pos in {'a', 'v'}:  # Adjective or Verb\n",
    "            return pos\n",
    "    return None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Perform text cleaning including lemmatization, filtering, and deduplication.\n",
    "    Always keep certain whitelisted words.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Ensure input is a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # Tokenize text and remove non-alphabetic words\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "\n",
    "    # Define a whitelist of words to always keep\n",
    "    whitelist = {'depression', 'affect', 'psychiatrist', 'suicide', 'worse','sad','disturbing','mental','commit','broken','break'}\n",
    "\n",
    "    # Lemmatize and filter adjectives and verbs\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        pos = get_wordnet_pos(word)\n",
    "        lemma = lemmatizer.lemmatize(word.lower(), pos) if pos else word.lower()\n",
    "        if pos and lemma not in whitelist:  # Keep only adjectives and verbs unless in whitelist\n",
    "            lemmatized_words.append(lemma)\n",
    "\n",
    "    # Add whitelisted words explicitly\n",
    "    lemmatized_words.extend(word for word in words if word.lower() in whitelist)\n",
    "\n",
    "    # Remove duplicates and words differing by a single letter\n",
    "    unique_words = set()\n",
    "    final_words = []\n",
    "    for word in lemmatized_words:\n",
    "        if not any(len(word) == len(other) and sum(c1 != c2 for c1, c2 in zip(word, other)) == 1 for other in unique_words):\n",
    "            unique_words.add(word)\n",
    "            final_words.append(word)\n",
    "\n",
    "    return ' '.join(sorted(set(final_words)))\n",
    "\n",
    "# Convert all entries in 'Post Title' to strings and apply the cleaning function\n",
    "df_post['Post Title'] = df_post['Post Title'].astype(str)\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(clean_text)\n",
    "\n",
    "df_comment['Comment'] = df_comment['Comment'].astype(str)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(clean_text)\n",
    "\n",
    "df_body['Post Body'] = df_body['Post Body'].astype(str)\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(clean_text)\n",
    "# combining into single data frame\n",
    "# df = pd.concat([df_post['Post Title'], df_comment['Comment']], axis=1)\n",
    "\n",
    "# Rename columns for clarity\n",
    "# df.columns = ['Post Title', 'Comment']\n",
    "\n",
    "# save the cleaned dataset to a new CSV file\n",
    "\n",
    "# df.to_csv('anxiety_cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51abd4fb-17c6-4ea5-8f2a-068c8982abdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>able accurate alleviate break compose connect design differentiate discourage emotional empower engage enjoy experience extend healthy helpful include informative intend involve mental mind mitigate participate please political practice productive provide remove send suicide trust volunteer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>able accurate alleviate break compose connect design differentiate discourage emotional empower engage enjoy experience extend healthy helpful include informative intend involve mental mind mitigate participate please political practice productive provide remove send suicide trust volunteer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                             Post Body\n",
       "0  able accurate alleviate break compose connect design differentiate discourage emotional empower engage enjoy experience extend healthy helpful include informative intend involve mental mind mitigate participate please political practice productive provide remove send suicide trust volunteer\n",
       "1  able accurate alleviate break compose connect design differentiate discourage emotional empower engage enjoy experience extend healthy helpful include informative intend involve mental mind mitigate participate please political practice productive provide remove send suicide trust volunteer"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_body.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906bada2-ab55-446c-8954-e1571b47a94e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
