{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1091c84-047d-424c-a302-ea3eeafa4c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "796daed2-837a-4488-a230-450f69659d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # Show all column content\n",
    "pd.set_option('display.max_columns', None)  # Show all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "433303b0-8c76-4f05-8e6c-e046e470187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/reddit_posts_comments_Depression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f82fe5ea-8d44-494f-b192-6eb406e4cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = df['Post Title']\n",
    "df_body = df['Post Body']\n",
    "df_comment = df['Comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be9a5b42-8f4b-4a57-b549-c6bffae2f923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_post = pd.DataFrame(df_post)\n",
    "df_post.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad764cce-140b-4137-9d41-43ff9ec805d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_body = pd.DataFrame(df_body)\n",
    "df_body.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f42d46a9-c105-4d7d-94ac-df5be6622814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_comment = pd.DataFrame(df_comment)\n",
    "df_comment.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "754b442d-f68c-4dd0-85b3-b13a8c7a6a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a934e33a-e386-4af9-8fe3-092fc7183aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(lambda m: m.strip().lower())\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(lambda m: m.strip().lower())\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(lambda m: m.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f71dfd9f-f7ad-4a8f-9a06-f5e3e964be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean and retain only words (strings)\n",
    "def clean_text(x):\n",
    "    if isinstance(x, str):  # Ensure it's a string\n",
    "        # Use regex to extract words, join them with spaces, and convert to lowercase\n",
    "        return \" \".join(re.findall(r'\\b\\w+', x))\n",
    "    return None  # Discard non-strings\n",
    "\n",
    "# Apply the cleaning function to the DataFrame columns\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(clean_text)\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(clean_text)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "657ef801-d17d-4a28-80c8-840d486e8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(lambda text: re.sub(r'\\s+', ' ',re.sub(r'_', ' ', re.sub(r'\\d+', '', text))).strip())\n",
    "\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(lambda text: re.sub(r'\\s+', ' ',re.sub(r'_', ' ', re.sub(r'\\d+', '', text))).strip())\n",
    "\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(lambda text: re.sub(r'\\s+', ' ',re.sub(r'_', ' ', re.sub(r'\\d+', '', text))).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71acb60a-b769-48f8-b225-fdac77b3c960",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10071783-6b8e-4cec-99de-d12c864b74ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea150e7a-f454-4f32-ac61-e2a41f3841a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e79aa2e-467c-478a-9668-7dae7365c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7f30fcd-ae18-4541-96ed-09c087ac5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(tokenizer.tokenize)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(tokenizer.tokenize)\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86d1d48e-666e-4735-a5bc-9fc2a89fa897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69aaba56-111e-42ab-9c12-623297c07f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\navne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a56b03c-fdf6-4c6b-9b27-143071dcba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a73e8f2-dfc5-4a76-abbb-22970489821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(message):\n",
    "    global stop_words\n",
    "    result = []\n",
    "    for word in str(message).split():  # Ensure the input is a string\n",
    "        if word.lower().strip() not in stop_words:\n",
    "            result.append(word)\n",
    "    return \" \".join(result)\n",
    "\n",
    "# Apply the function to each entry in the Post Body column\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)  # Convert lists to strings\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(remove_stopwords)\n",
    "\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)  # Convert lists to strings\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(remove_stopwords)\n",
    "\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)  # Convert lists to strings\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52c1556e-25fa-45ef-9897-6d49ad69a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "802a44ad-56ca-4975-9871-1959a2a5f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41c4e1e5-d00a-4e0d-8a6c-1e1da16c29ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba96bf25-5133-4a01-ae06-2dc19a3e2ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(tokenizer.tokenize)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(tokenizer.tokenize)\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "918e5453-f855-4ddc-a802-70bca527fe2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[understood, apologise, forget, future, break]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[biggest, problem, private, talks, may, may, reach, point, advice, one, needs, help, leave, chat, would, even, make, worse, public, chat, good, chance, others, may, jump, got, lost]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                 Comment\n",
       "0                                                                                                                                         [understood, apologise, forget, future, break]\n",
       "1                                                                                                                                                                              [removed]\n",
       "2  [biggest, problem, private, talks, may, may, reach, point, advice, one, needs, help, leave, chat, would, even, make, worse, public, chat, good, chance, others, may, jump, got, lost]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comment.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b945abba-455b-46ca-a8f3-df9636006e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment = df_comment[df_comment[\"Comment\"].apply(lambda x: x != [\"removed\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea429362-11c0-49bc-8462-cb7a65205281",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment = df_comment[df_comment[\"Comment\"].apply(lambda x: x != [\"deleted\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ab08ca1-fc70-4ad6-80a0-b8b21d4d7a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_comment.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "749e1816-66e1-4fce-a8c0-fa87a15bd0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = df_post.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc2aad79-0448-4abf-8bf4-b43322702266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_post = df_post.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "609c0e9c-59f6-42a2-b263-e2fba921053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8b7c5e6-9981-4b42-af44-5c57ee9b0def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"\n",
    "    Map POS tag to first character lemmatize() accepts.\n",
    "    Only adjectives (a) and verbs (v) are considered.\n",
    "    \"\"\"\n",
    "    tag = wordnet.synsets(word)\n",
    "    if tag:\n",
    "        pos = tag[0].pos()\n",
    "        if pos in {'a', 'v','n'}:  # Adjective or Verb\n",
    "            return pos\n",
    "    return None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Perform text cleaning including lemmatization, filtering, and deduplication.\n",
    "    Always keep certain whitelisted words.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Ensure input is a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # Tokenize text and remove non-alphabetic words\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "\n",
    "    # Define a whitelist of words to always keep\n",
    "    whitelist = {'depression', 'affect', 'psychiatrist', 'suicide', 'worse','sad','disturbing','mental','commit','broken','break'}\n",
    "\n",
    "    # Lemmatize and filter adjectives and verbs\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        pos = get_wordnet_pos(word)\n",
    "        lemma = lemmatizer.lemmatize(word.lower(), pos) if pos else word.lower()\n",
    "        if pos and lemma not in whitelist:  # Keep only adjectives and verbs unless in whitelist\n",
    "            lemmatized_words.append(lemma)\n",
    "\n",
    "    # Add whitelisted words explicitly\n",
    "    lemmatized_words.extend(word for word in words if word.lower() in whitelist)\n",
    "\n",
    "    # Remove duplicates and words differing by a single letter\n",
    "    unique_words = set()\n",
    "    final_words = []\n",
    "    for word in lemmatized_words:\n",
    "        if not any(len(word) == len(other) and sum(c1 != c2 for c1, c2 in zip(word, other)) == 1 for other in unique_words):\n",
    "            unique_words.add(word)\n",
    "            final_words.append(word)\n",
    "\n",
    "    return ' '.join(sorted(set(final_words)))\n",
    "\n",
    "# Convert all entries in 'Post Title' to strings and apply the cleaning function\n",
    "df_post['Post Title'] = df_post['Post Title'].astype(str)\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(clean_text)\n",
    "\n",
    "df_comment['Comment'] = df_comment['Comment'].astype(str)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(clean_text)\n",
    "\n",
    "df_body['Post Body'] = df_body['Post Body'].astype(str)\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(clean_text)\n",
    "\n",
    "# combining into single data frame\n",
    "# df = pd.concat([df_post['Post Title'],df_body['Post Body'], df_comment['Comment']], axis=1)\n",
    "\n",
    "# Rename columns for clarity\n",
    "# df.columns = ['Post Title','Post Body', 'Comment']\n",
    "\n",
    "# save the cleaned dataset to a new CSV file\n",
    "\n",
    "# df.to_csv('depression_cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67502617-93f2-48cf-9e91-e633f654757f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b40e0938-c7a8-4786-bfdc-cc1a449a79db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_post_train,X_post_test = train_test_split(\n",
    "    df_post['Post Title'],test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d32df47-a6e6-43ee-a9ea-c445ad6546fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_body_train,X_body_test = train_test_split(\n",
    "    df_body['Post Body'],test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2af4a491-1e8c-4624-ba3b-32a24e1cd4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_comment_train,X_comment_test = train_test_split(\n",
    "    df_comment['Comment'],test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6558fe3a-138a-4c33-8e88-bd4616547a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize all datasets\n",
    "tokenized_post_train = [word_tokenize(post.lower()) for post in X_post_train]\n",
    "tokenized_body_train = [word_tokenize(body.lower()) for body in X_body_train]\n",
    "tokenized_comment_train = [word_tokenize(comment.lower()) for comment in X_comment_train]\n",
    "\n",
    "# Combine all tokenized data for Word2Vec training\n",
    "combined_tokenized_data = tokenized_post_train + tokenized_body_train + tokenized_comment_train\n",
    "\n",
    "# Train Word2Vec model on the combined data\n",
    "model = Word2Vec(\n",
    "    sentences=combined_tokenized_data,\n",
    "    vector_size=200,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "model.save('word2vec_model')\n",
    "\n",
    "# Function to compute average Word2Vec vectors\n",
    "def avg_word2vec(sentence, model, vector_size):\n",
    "    word_vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Compute average vectors for each dataset\n",
    "avg_vectors_post = np.array([avg_word2vec(sentence, model, 200) for sentence in tokenized_post_train])\n",
    "avg_vectors_body = np.array([avg_word2vec(sentence, model, 200) for sentence in tokenized_body_train])\n",
    "avg_vectors_comment = np.array([avg_word2vec(sentence, model, 200) for sentence in tokenized_comment_train])\n",
    "\n",
    "# Print shapes of the resulting vectors\n",
    "print(\"Shape of average vectors for posts:\", avg_vectors_post.shape)\n",
    "print(\"Shape of average vectors for bodies:\", avg_vectors_body.shape)\n",
    "print(\"Shape of average vectors for comments:\", avg_vectors_comment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "241476d6-fcf1-43f7-9d94-68cd0292e5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of average Word2Vec vectors: (244, 200)\n"
     ]
    }
   ],
   "source": [
    "# Function to compute average Word2Vec vector for a sentence\n",
    "def avg_word2vec(sentence, model, vector_size):\n",
    "    # Filter out words not in the model vocabulary\n",
    "    word_vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    \n",
    "    # If no words in the model, return a zero vector\n",
    "    if not word_vectors:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "    # Compute the average\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Compute average Word2Vec vectors for the dataset\n",
    "avg_vectors = np.array([avg_word2vec(sentence, model, model.vector_size) for sentence in tokenized_sentences])\n",
    "\n",
    "# Print the shape of the resulting matrix\n",
    "print(\"Shape of average Word2Vec vectors:\", avg_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bef05fd0-0b96-4e0d-b59d-636c256f5b96",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m avg_word2vec(tokenized_sentences_post,model,\u001b[38;5;241m200\u001b[39m)\n",
      "Cell \u001b[1;32mIn[51], line 4\u001b[0m, in \u001b[0;36mavg_word2vec\u001b[1;34m(sentence, model, vector_size)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mavg_word2vec\u001b[39m(sentence, model, vector_size):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Filter out words not in the model vocabulary\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     word_vectors \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39mwv[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mwv]\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# If no words in the model, return a zero vector\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m word_vectors:\n",
      "Cell \u001b[1;32mIn[51], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mavg_word2vec\u001b[39m(sentence, model, vector_size):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Filter out words not in the model vocabulary\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     word_vectors \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39mwv[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mwv]\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# If no words in the model, return a zero vector\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m word_vectors:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:649\u001b[0m, in \u001b[0;36mKeyedVectors.__contains__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__contains__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:646\u001b[0m, in \u001b[0;36mKeyedVectors.has_index_for\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhas_index_for\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m    638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can this model return a single index for this key?\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \n\u001b[0;32m    640\u001b[0m \u001b[38;5;124;03m    Subclasses that synthesize vectors for out-of-vocabulary words (like\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    644\u001b[0m \n\u001b[0;32m    645\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 646\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:412\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_index\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    408\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the integer index (slot/position) where the given key's vector is stored in the\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;124;03m    backing vectors array.\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \n\u001b[0;32m    411\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 412\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_to_index\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "avg_word2vec(tokenized_sentences_post,model,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a6b0f3-71e7-409b-9359-f283a3b8b491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759fd2d-33b6-406d-9399-a10ba02ef09e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d04af-525b-4ee2-aa6a-e012791bdb48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1770fd20-932c-49e6-b701-e00331dddb3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3760b2c0-fd49-4b8f-b43d-cf2f78b7f493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf205c86-2a77-4e53-ae2a-fc37bf9021a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0489107b-932d-4dd7-b374-d062eb3963d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=16000,binary=True,ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c13ab9d-1347-424b-9768-fad1287c9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_post = cv.fit_transform(df_post['Post Title'])\n",
    "X_body = cv.fit_transform(df_body['Post Body'])\n",
    "X_comment = cv.fit_transform(df_comment['Comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d97ba9-0fb6-4dd7-8c81-0a84c0769d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_post.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ce4b8-201b-4c32-bd45-ecf17ad42bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_body.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ffbab-a74d-4b39-8983-fe1573ff47b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_comment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae1b097-e7b8-497a-b648-fdcb665ef3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(max_features=16000, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50298366-2fa6-4f51-988e-2559df7ccad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p = tf.fit_transform(df_post['Post Title'])\n",
    "X_b = tf.fit_transform(df_body['Post Body'])\n",
    "X_c = tf.fit_transform(df_comment['Comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f2b01-2786-4113-b231-82e00b10d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a636619-7371-4833-b294-0a2d22b70b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a772d82-a2ab-43da-853a-01aec4e1c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58b5688-d82c-4570-9ed4-c47261461b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
