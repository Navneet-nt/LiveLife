{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1091c84-047d-424c-a302-ea3eeafa4c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "796daed2-837a-4488-a230-450f69659d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # Show all column content\n",
    "pd.set_option('display.max_columns', None)  # Show all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "433303b0-8c76-4f05-8e6c-e046e470187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"datasets/reddit_posts_comments_Depression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74538615-1fea-4630-a99a-a73799c56bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv(\"datasets/reddit_posts_comments_Anxiety.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef0cb63-5902-4564-a901-3ef5a3841bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_post_1, df_body_1, df_comment_1 are from the first source\n",
    "# and df_post_2, df_body_2, df_comment_2 are from the second source\n",
    "\n",
    "# Create dataframes from each source\n",
    "df_1 = pd.DataFrame({\n",
    "    'Post Title': df_1['Post Title'],\n",
    "    'Post Body': df_1['Post Body'],\n",
    "    'Comment': df_1['Comment']\n",
    "})\n",
    "\n",
    "df_2 = pd.DataFrame({\n",
    "    'Post Title': df_2['Post Title'],\n",
    "    'Post Body': df_2['Post Body'],\n",
    "    'Comment': df_2['Comment']\n",
    "})\n",
    "\n",
    "# Concatenate the two datasets\n",
    "df = pd.concat([df_1, df_2], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "808c6221-c495-4095-bb0a-dd7eaf5fc825",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = df['Post Title']\n",
    "df_body = df['Post Body']\n",
    "df_comment = df['Comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be9a5b42-8f4b-4a57-b549-c6bffae2f923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_post = pd.DataFrame(df_post)\n",
    "df_post.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad764cce-140b-4137-9d41-43ff9ec805d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_body = pd.DataFrame(df_body)\n",
    "df_body.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f42d46a9-c105-4d7d-94ac-df5be6622814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_comment = pd.DataFrame(df_comment)\n",
    "df_comment.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "754b442d-f68c-4dd0-85b3-b13a8c7a6a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a934e33a-e386-4af9-8fe3-092fc7183aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all values are strings before applying strip() and lower()\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(lambda m: str(m).strip().lower() if isinstance(m, str) else \"\")\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(lambda m: str(m).strip().lower() if isinstance(m, str) else \"\")\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(lambda m: str(m).strip().lower() if isinstance(m, str) else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f71dfd9f-f7ad-4a8f-9a06-f5e3e964be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean and retain only words (strings)\n",
    "def clean_text(x):\n",
    "    if isinstance(x, str):  # Ensure it's a string\n",
    "        # Use regex to extract words, join them with spaces, and convert to lowercase\n",
    "        return \" \".join(re.findall(r'\\b\\w+', x))\n",
    "    return None  # Discard non-strings\n",
    "\n",
    "# Apply the cleaning function to the DataFrame columns\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(clean_text)\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(clean_text)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "657ef801-d17d-4a28-80c8-840d486e8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(lambda text: re.sub(r'\\s+', ' ',re.sub(r'_', ' ', re.sub(r'\\d+', '', text))).strip())\n",
    "\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(lambda text: re.sub(r'\\s+', ' ',re.sub(r'_', ' ', re.sub(r'\\d+', '', text))).strip())\n",
    "\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(lambda text: re.sub(r'\\s+', ' ',re.sub(r'_', ' ', re.sub(r'\\d+', '', text))).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71acb60a-b769-48f8-b225-fdac77b3c960",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10071783-6b8e-4cec-99de-d12c864b74ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea150e7a-f454-4f32-ac61-e2a41f3841a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e79aa2e-467c-478a-9668-7dae7365c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7f30fcd-ae18-4541-96ed-09c087ac5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(tokenizer.tokenize)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(tokenizer.tokenize)\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86d1d48e-666e-4735-a5bc-9fc2a89fa897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69aaba56-111e-42ab-9c12-623297c07f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\navne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a56b03c-fdf6-4c6b-9b27-143071dcba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a73e8f2-dfc5-4a76-abbb-22970489821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(message):\n",
    "    global stop_words\n",
    "    result = []\n",
    "    for word in str(message).split():  # Ensure the input is a string\n",
    "        if word.lower().strip() not in stop_words:\n",
    "            result.append(word)\n",
    "    return \" \".join(result)\n",
    "\n",
    "# Apply the function to each entry in the Post Body column\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)  # Convert lists to strings\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(remove_stopwords)\n",
    "\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)  # Convert lists to strings\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(remove_stopwords)\n",
    "\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)  # Convert lists to strings\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52c1556e-25fa-45ef-9897-6d49ad69a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "802a44ad-56ca-4975-9871-1959a2a5f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41c4e1e5-d00a-4e0d-8a6c-1e1da16c29ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba96bf25-5133-4a01-ae06-2dc19a3e2ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(tokenizer.tokenize)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(tokenizer.tokenize)\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "918e5453-f855-4ddc-a802-70bca527fe2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[understood, apologise, forget, future, break]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[biggest, problem, private, talks, may, may, reach, point, advice, one, needs, help, leave, chat, would, even, make, worse, public, chat, good, chance, others, may, jump, got, lost]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                 Comment\n",
       "0                                                                                                                                         [understood, apologise, forget, future, break]\n",
       "1                                                                                                                                                                              [removed]\n",
       "2  [biggest, problem, private, talks, may, may, reach, point, advice, one, needs, help, leave, chat, would, even, make, worse, public, chat, good, chance, others, may, jump, got, lost]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comment.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b945abba-455b-46ca-a8f3-df9636006e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment = df_comment[df_comment[\"Comment\"].apply(lambda x: x != [\"removed\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea429362-11c0-49bc-8462-cb7a65205281",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment = df_comment[df_comment[\"Comment\"].apply(lambda x: x != [\"deleted\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ab08ca1-fc70-4ad6-80a0-b8b21d4d7a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_comment.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "749e1816-66e1-4fce-a8c0-fa87a15bd0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = df_post.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc2aad79-0448-4abf-8bf4-b43322702266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_post = df_post.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "609c0e9c-59f6-42a2-b263-e2fba921053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8b7c5e6-9981-4b42-af44-5c57ee9b0def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"\n",
    "    Map POS tag to first character lemmatize() accepts.\n",
    "    Only adjectives (a) and verbs (v) are considered.\n",
    "    \"\"\"\n",
    "    tag = wordnet.synsets(word)\n",
    "    if tag:\n",
    "        pos = tag[0].pos()\n",
    "        if pos in {'a', 'v','n'}:  # Adjective or Verb\n",
    "            return pos\n",
    "    return None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Perform text cleaning including lemmatization, filtering, and deduplication.\n",
    "    Always keep certain whitelisted words.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Ensure input is a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # Tokenize text and remove non-alphabetic words\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "\n",
    "    # Define a whitelist of words to always keep\n",
    "    whitelist = {'depression', 'affect', 'psychiatrist', 'suicide', 'worse','sad','disturbing','mental','commit','broken','break'}\n",
    "\n",
    "    # Lemmatize and filter adjectives and verbs\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        pos = get_wordnet_pos(word)\n",
    "        lemma = lemmatizer.lemmatize(word.lower(), pos) if pos else word.lower()\n",
    "        if pos and lemma not in whitelist:  # Keep only adjectives and verbs unless in whitelist\n",
    "            lemmatized_words.append(lemma)\n",
    "\n",
    "    # Add whitelisted words explicitly\n",
    "    lemmatized_words.extend(word for word in words if word.lower() in whitelist)\n",
    "\n",
    "    # Remove duplicates and words differing by a single letter\n",
    "    unique_words = set()\n",
    "    final_words = []\n",
    "    for word in lemmatized_words:\n",
    "        if not any(len(word) == len(other) and sum(c1 != c2 for c1, c2 in zip(word, other)) == 1 for other in unique_words):\n",
    "            unique_words.add(word)\n",
    "            final_words.append(word)\n",
    "\n",
    "    return ' '.join(sorted(set(final_words)))\n",
    "\n",
    "# Convert all entries in 'Post Title' to strings and apply the cleaning function\n",
    "df_post['Post Title'] = df_post['Post Title'].astype(str)\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(clean_text)\n",
    "\n",
    "df_comment['Comment'] = df_comment['Comment'].astype(str)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(clean_text)\n",
    "\n",
    "df_body['Post Body'] = df_body['Post Body'].astype(str)\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(clean_text)\n",
    "\n",
    "# combining into single data frame\n",
    "# df = pd.concat([df_post['Post Title'],df_body['Post Body'], df_comment['Comment']], axis=1)\n",
    "\n",
    "# Rename columns for clarity\n",
    "# df.columns = ['Post Title','Post Body', 'Comment']\n",
    "\n",
    "# save the cleaned dataset to a new CSV file\n",
    "\n",
    "# df.to_csv('depression_cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b40e0938-c7a8-4786-bfdc-cc1a449a79db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_post_train,X_post_test = train_test_split(\n",
    "    df_post['Post Title'],test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d32df47-a6e6-43ee-a9ea-c445ad6546fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_body_train,X_body_test = train_test_split(\n",
    "    df_body['Post Body'],test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2af4a491-1e8c-4624-ba3b-32a24e1cd4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_comment_train,X_comment_test = train_test_split(\n",
    "    df_comment['Comment'],test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6558fe3a-138a-4c33-8e88-bd4616547a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of average vectors for posts: (569, 200)\n",
      "Shape of average vectors for bodies: (4111, 200)\n",
      "Shape of average vectors for comments: (4066, 200)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize all datasets\n",
    "tokenized_post_train = [word_tokenize(post.lower()) for post in X_post_train]\n",
    "tokenized_body_train = [word_tokenize(body.lower()) for body in X_body_train]\n",
    "tokenized_comment_train = [word_tokenize(comment.lower()) for comment in X_comment_train]\n",
    "\n",
    "# Combine all tokenized data for Word2Vec training\n",
    "combined_tokenized_data = tokenized_post_train + tokenized_body_train + tokenized_comment_train\n",
    "\n",
    "# Train Word2Vec model on the combined data\n",
    "model = Word2Vec(\n",
    "    sentences=combined_tokenized_data,\n",
    "    vector_size=200,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "model.save('word2vec_model')\n",
    "\n",
    "# Function to compute average Word2Vec vectors\n",
    "def avg_word2vec(sentence, model, vector_size):\n",
    "    word_vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Compute average vectors for each dataset\n",
    "avg_vectors_post = np.array([avg_word2vec(sentence, model, 200) for sentence in tokenized_post_train])\n",
    "avg_vectors_body = np.array([avg_word2vec(sentence, model, 200) for sentence in tokenized_body_train])\n",
    "avg_vectors_comment = np.array([avg_word2vec(sentence, model, 200) for sentence in tokenized_comment_train])\n",
    "\n",
    "# Print shapes of the resulting vectors\n",
    "print(\"Shape of average vectors for posts:\", avg_vectors_post.shape)\n",
    "print(\"Shape of average vectors for bodies:\", avg_vectors_body.shape)\n",
    "print(\"Shape of average vectors for comments:\", avg_vectors_comment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d054781-71e7-4a3e-83d4-e8296a226927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Combined Features: (4111, 600)\n"
     ]
    }
   ],
   "source": [
    "# combinig vectors\n",
    "import numpy as np\n",
    "\n",
    "# Determine the maximum number of samples\n",
    "max_samples = max(len(avg_vectors_post), len(avg_vectors_body), len(avg_vectors_comment))\n",
    "\n",
    "# Define a zero vector for padding\n",
    "zero_vector = np.zeros((200,))\n",
    "\n",
    "# Pad the smaller datasets with zero vectors\n",
    "padded_posts = np.vstack([\n",
    "    avg_vectors_post,\n",
    "    np.tile(zero_vector, (max_samples - len(avg_vectors_post), 1))\n",
    "])\n",
    "\n",
    "padded_bodies = np.vstack([\n",
    "    avg_vectors_body,\n",
    "    np.tile(zero_vector, (max_samples - len(avg_vectors_body), 1))\n",
    "])\n",
    "\n",
    "padded_comments = np.vstack([\n",
    "    avg_vectors_comment,\n",
    "    np.tile(zero_vector, (max_samples - len(avg_vectors_comment), 1))\n",
    "])\n",
    "\n",
    "# Combine the padded datasets into one\n",
    "combined_features = np.hstack([padded_posts, padded_bodies, padded_comments])\n",
    "\n",
    "print(\"Shape of Combined Features:\", combined_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b91c784a-c513-48d6-b1dc-597de0b851ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m features_tensor \u001b[38;5;241m=\u001b[39m features_tensor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m)  \u001b[38;5;66;03m# Reshape to (batch, channel, height, width)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Create DataLoader\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TensorDataset(features_tensor, labels_tensor)\n\u001b[0;32m     21\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Define the CNN model\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:205\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[1;34m(self, *tensors)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m    206\u001b[0m         tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors\n\u001b[0;32m    207\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "\u001b[1;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Sample combined_features for testing\n",
    "# Replace with your actual combined_features array and labels\n",
    "#combined_features = np.random.rand(100, 600)  # Example shape\n",
    "labels = np.random.randint(0, 3, 100)  # 0: Neutral, 1: Anxious, 2: Depressed\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "features_tensor = torch.tensor(combined_features, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Ensure the features have a shape suitable for a CNN (simulate a 2D structure)\n",
    "features_tensor = features_tensor.view(-1, 1, 20, 30)  # Reshape to (batch, channel, height, width)\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the CNN model\n",
    "class DepressionClassifierCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DepressionClassifierCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Dynamically compute the flattened size for the fully connected layer\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.zeros(1, 1, 20, 30)  # Simulated input\n",
    "            sample_output = self.pool(self.conv2(self.pool(self.conv1(sample_input))))\n",
    "            self.flattened_size = sample_output.view(-1).size(0)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)  # 3 classes: Neutral, Anxious, Depressed\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = DepressionClassifierCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for features, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"depression_classifier_cnn.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf501f9-8c1a-4ad8-848e-302a3a96cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dhjgkkyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc462326-8515-4203-894c-3599d74e4d98",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Feature and label counts must match.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m features_tensor \u001b[38;5;241m=\u001b[39m features_tensor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, height, width)  \u001b[38;5;66;03m# Reshape to (batch, channel, height, width)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Ensure labels_tensor matches the batch size\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m features_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m labels_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature and label counts must match.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Create DataLoader\u001b[39;00m\n\u001b[0;32m     28\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TensorDataset(features_tensor, labels_tensor)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Feature and label counts must match."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Sample combined_features for testing\n",
    "# combined_features = np.random.rand(100, 600)  # Example shape\n",
    "labels = np.random.randint(0, 3, 100)  # 0: Neutral, 1: Anxious, 2: Depressed\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "features_tensor = torch.tensor(combined_features, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Calculate the height and width dynamically\n",
    "total_features = features_tensor.size(1)  # Number of features (600 in this case)\n",
    "height = 20  # Choose a fixed height\n",
    "width = total_features // height  # Width is determined by dividing total features by height\n",
    "assert total_features == height * width, \"Height and width must evenly divide the total features.\"\n",
    "\n",
    "# Reshape features_tensor to match CNN input requirements\n",
    "features_tensor = features_tensor.view(-1, 1, height, width)  # Reshape to (batch, channel, height, width)\n",
    "\n",
    "# Ensure labels_tensor matches the batch size\n",
    "assert features_tensor.size(0) == labels_tensor.size(0), \"Feature and label counts must match.\"\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the CNN model\n",
    "class DepressionClassifierCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DepressionClassifierCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Dynamically compute the flattened size for the fully connected layer\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.zeros(1, 1, height, width)  # Simulated input\n",
    "            sample_output = self.pool(self.conv2(self.pool(self.conv1(sample_input))))\n",
    "            self.flattened_size = sample_output.view(-1).size(0)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)  # 3 classes: Neutral, Anxious, Depressed\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = DepressionClassifierCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for features, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"depression_classifier_cnn.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59b588ff-d689-4250-b6a0-6d258a56a6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4111"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab9023f8-767f-443b-b912-658d843b54d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3ccc739-a64b-4483-82a9-8fe5b8e59ffb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x1120 and 4800x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m features, label \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     65\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 66\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(features)\n\u001b[0;32m     67\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[0;32m     68\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[45], line 51\u001b[0m, in \u001b[0;36mDepressionClassifierCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n\u001b[0;32m     50\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[0;32m     52\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x1120 and 4800x128)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Sample combined_features for testing\n",
    "# Replace with your actual combined_features array and labels\n",
    "# combined_features = np.random.rand(4111, 600)  # Example shape\n",
    "labels = np.random.randint(0, 3, 100)  # 0: Neutral, 1: Anxious, 2: Depressed\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "features_tensor = torch.tensor(combined_features, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Match the sizes of the tensors\n",
    "min_size = min(features_tensor.size(0), labels_tensor.size(0))\n",
    "features_tensor = features_tensor[:min_size]\n",
    "labels_tensor = labels_tensor[:min_size]\n",
    "\n",
    "# Ensure sizes match after adjustment\n",
    "assert features_tensor.size(0) == labels_tensor.size(0), \"Features and labels must match in size.\"\n",
    "\n",
    "# Reshape the features tensor\n",
    "height = 20\n",
    "width = features_tensor.size(1) // height\n",
    "features_tensor = features_tensor.view(-1, 1, height, width)  # Reshape to (batch, channel, height, width)\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the CNN model\n",
    "class DepressionClassifierCNN(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(DepressionClassifierCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear((height // 2) * (width // 2) * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = DepressionClassifierCNN(input_channels=1, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for features, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"depression_classifier_cnn.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f3950a2-b243-42f1-a04a-9cf6299d3136",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x1120 and 192x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m features, label \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     72\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 73\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(features)\n\u001b[0;32m     74\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[0;32m     75\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[46], line 58\u001b[0m, in \u001b[0;36mDepressionClassifierCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n\u001b[0;32m     57\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[0;32m     59\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x1120 and 192x128)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Sample combined_features for testing\n",
    "# Replace with your actual combined_features array and labels\n",
    "# combined_features = np.random.rand(4111, 600)  # Example shape\n",
    "labels = np.random.randint(0, 3, 100)  # 0: Neutral, 1: Anxious, 2: Depressed\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "features_tensor = torch.tensor(combined_features, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Match the sizes of the tensors\n",
    "min_size = min(features_tensor.size(0), labels_tensor.size(0))\n",
    "features_tensor = features_tensor[:min_size]\n",
    "labels_tensor = labels_tensor[:min_size]\n",
    "\n",
    "# Ensure sizes match after adjustment\n",
    "assert features_tensor.size(0) == labels_tensor.size(0), \"Features and labels must match in size.\"\n",
    "\n",
    "# Reshape the features tensor\n",
    "height = 20\n",
    "width = features_tensor.size(1) // height\n",
    "features_tensor = features_tensor.view(-1, 1, height, width)  # Reshape to (batch, channel, height, width)\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the CNN model\n",
    "class DepressionClassifierCNN(nn.Module):\n",
    "    def __init__(self, input_channels, height, width, num_classes):\n",
    "        super(DepressionClassifierCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Calculate dimensions after convolution and pooling\n",
    "        dummy_input = torch.zeros(1, input_channels, height, width)\n",
    "        with torch.no_grad():\n",
    "            out = self.pool(self.pool(self.conv2(self.pool(self.conv1(dummy_input)))))\n",
    "            flattened_dim = out.numel()\n",
    "        \n",
    "        self.fc1 = nn.Linear(flattened_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = DepressionClassifierCNN(input_channels=1, height=height, width=width, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for features, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"depression_classifier_cnn.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0909608f-ebc6-485c-bbf3-825f19124e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4111, 600)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d4dca63-6a7c-4d8b-aef5-fb66c32789c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x1120 and 192x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m features, label \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     72\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 73\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(features)\n\u001b[0;32m     74\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[0;32m     75\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[50], line 58\u001b[0m, in \u001b[0;36mDepressionClassifierCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n\u001b[0;32m     57\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[0;32m     59\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x1120 and 192x128)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Sample combined_features for testing\n",
    "# Replace with your actual combined_features array and labels\n",
    "# combined_features = np.random.rand(4111, 600)  # Example shape\n",
    "labels = np.random.randint(0, 3, 100)  # 0: Neutral, 1: Anxious, 2: Depressed\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "features_tensor = torch.tensor(combined_features, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Match the sizes of the tensors\n",
    "min_size = min(features_tensor.size(0), labels_tensor.size(0))\n",
    "features_tensor = features_tensor[:min_size]\n",
    "labels_tensor = labels_tensor[:min_size]\n",
    "\n",
    "# Ensure sizes match after adjustment\n",
    "assert features_tensor.size(0) == labels_tensor.size(0), \"Features and labels must match in size.\"\n",
    "\n",
    "# Reshape the features tensor\n",
    "height = 20\n",
    "width = features_tensor.size(1) // height\n",
    "features_tensor = features_tensor.view(-1, 1, height, width)  # Reshape to (batch, channel, height, width)\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the CNN model\n",
    "class DepressionClassifierCNN(nn.Module):\n",
    "    def __init__(self, input_channels, height, width, num_classes):\n",
    "        super(DepressionClassifierCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Calculate dimensions after convolution and pooling\n",
    "        dummy_input = torch.zeros(1, input_channels, height, width)\n",
    "        with torch.no_grad():\n",
    "            out = self.pool(self.pool(self.conv2(self.pool(self.conv1(dummy_input)))))\n",
    "            flattened_dim = out.numel()  # Total features after convolution and pooling\n",
    "        \n",
    "        self.fc1 = nn.Linear(flattened_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = DepressionClassifierCNN(input_channels=1, height=height, width=width, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for features, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"depression_classifier_cnn.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0c8f761-ccd7-4f13-b467-235335f01429",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x1120 and 192x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m features, label \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     75\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 76\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(features)\n\u001b[0;32m     77\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[0;32m     78\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[51], line 61\u001b[0m, in \u001b[0;36mDepressionClassifierCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     59\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n\u001b[0;32m     60\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[0;32m     62\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x1120 and 192x128)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define combined_features and labels\n",
    "# combined_features = np.random.rand(4111, 600)  # Replace with your actual data\n",
    "labels = np.random.randint(0, 3, 100)  # Example labels\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "features_tensor = torch.tensor(combined_features, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Adjust sizes to match\n",
    "if features_tensor.size(0) > labels_tensor.size(0):\n",
    "    # Pad labels tensor with random labels to match features\n",
    "    extra_labels = torch.randint(0, 3, (features_tensor.size(0) - labels_tensor.size(0),))\n",
    "    labels_tensor = torch.cat((labels_tensor, extra_labels), dim=0)\n",
    "elif features_tensor.size(0) < labels_tensor.size(0):\n",
    "    # Truncate labels tensor to match features\n",
    "    labels_tensor = labels_tensor[:features_tensor.size(0)]\n",
    "\n",
    "# Ensure sizes match after adjustment\n",
    "assert features_tensor.size(0) == labels_tensor.size(0), \"Features and labels must match in size.\"\n",
    "\n",
    "# Reshape the features tensor for CNN input\n",
    "height = 20\n",
    "width = features_tensor.size(1) // height\n",
    "features_tensor = features_tensor.view(-1, 1, height, width)  # Reshape to (batch, channel, height, width)\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the CNN model\n",
    "class DepressionClassifierCNN(nn.Module):\n",
    "    def __init__(self, input_channels, height, width, num_classes):\n",
    "        super(DepressionClassifierCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Calculate dimensions after convolution and pooling\n",
    "        dummy_input = torch.zeros(1, input_channels, height, width)\n",
    "        with torch.no_grad():\n",
    "            out = self.pool(self.pool(self.conv2(self.pool(self.conv1(dummy_input)))))\n",
    "            flattened_dim = out.numel()  # Total features after convolution and pooling\n",
    "        \n",
    "        self.fc1 = nn.Linear(flattened_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = DepressionClassifierCNN(input_channels=1, height=height, width=width, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for features, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"depression_classifier_cnn.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b2f78-3ca6-4aad-ab27-6d175edc63bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1876c3e-8185-4fd2-98f1-172fdef1def4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b51eca-f2b4-4863-8497-811d86041834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80878daf-80a8-4b4a-8140-aa42b2c1a76e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0216e440-6461-40ab-9b87-c47a48932c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0567fe28-2dca-4130-b346-f0f81ba0d440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210920bb-9002-4b1d-bbe2-ec5b902d8939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer, losses, models\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Custom Dataset class\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, combined_features):\n",
    "        self.combined_features = torch.tensor(combined_features, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.combined_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.combined_features[idx]\n",
    "        return feature, feature  # Modify for real contrastive pairs\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = EmbeddingDataset(combined_features)\n",
    "train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define the custom model\n",
    "class CustomEmbeddingModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=600):\n",
    "        super(CustomEmbeddingModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 600),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(600, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = combined_features.shape[1]  # Combined feature dimension\n",
    "model = CustomEmbeddingModel(input_dim=600)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = losses.MultipleNegativesRankingLoss(model=model)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get input and target features\n",
    "        input_features, target_features = batch\n",
    "        input_features = input_features.float()\n",
    "        target_features = target_features.float()\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings = model(input_features)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(embeddings, target_features)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_dataloader):.4f}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"custom_embedding_model.pth\")\n",
    "print(\"Model training complete and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834d2179-2fdf-481c-9cdd-6848627655b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer, models, losses\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Assume combined_features is a NumPy array with shape (num_samples, 600)\n",
    "combined_features = np.random.rand(100, 600)  # Replace with your actual data\n",
    "\n",
    "# Step 1: Create a custom dataset\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        return feature, feature  # Return the feature as both input and label for contrastive learning\n",
    "\n",
    "# Step 2: Create DataLoader\n",
    "dataset = EmbeddingDataset(combined_features)\n",
    "train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Step 3: Define a simple custom model to process the combined embeddings\n",
    "class CustomEmbeddingModel(SentenceTransformer):\n",
    "    def __init__(self, combined_feature_dim):\n",
    "        # First, initialize the parent class\n",
    "        super().__init__()\n",
    "\n",
    "        # Define a simple MLP that processes the combined embeddings\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(combined_feature_dim, 768),  # Adjust based on your input feature size\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(768, 768),  # Keep embedding dimension as 768\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.model(features)\n",
    "\n",
    "# Step 4: Initialize the model and loss function\n",
    "base_model = CustomEmbeddingModel(combined_feature_dim=600)  # 600 is the input size (combined_features)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=base_model)\n",
    "\n",
    "# Step 5: Train the model using the DataLoader\n",
    "base_model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,  # Number of epochs\n",
    "    warmup_steps=100,  # Warmup steps\n",
    "    show_progress_bar=True  # Show progress bar\n",
    ")\n",
    "\n",
    "# Step 6: Calculate similarities after training\n",
    "# Assuming the model is trained and you want to compute similarities between embeddings\n",
    "embeddings = base_model.encode(combined_features)\n",
    "\n",
    "# Calculate cosine similarities between all embeddings (for example, between all rows of `combined_features`)\n",
    "from sentence_transformers import util\n",
    "similarities = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "# Print out the similarities\n",
    "print(similarities.shape)  # This should print [num_samples, num_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26587d51-85cb-40d0-86e9-363435af1427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sentence_transformers import SentenceTransformer, models, losses\n",
    "import torch.nn as nn\n",
    "\n",
    "# Step 1: Split combined_features into separate components\n",
    "padded_posts = combined_features[:, :200]    # First 200 features for posts\n",
    "padded_bodies = combined_features[:, 200:400]  # Next 200 features for bodies\n",
    "padded_comments = combined_features[:, 400:]   # Last 200 features for comments\n",
    "\n",
    "# Combine them into a list of tuples, where each element is a tuple of (post, body, comment) embeddings\n",
    "restructured_features = [\n",
    "    (padded_posts[i], padded_bodies[i], padded_comments[i])\n",
    "    for i in range(combined_features.shape[0])\n",
    "]\n",
    "\n",
    "# Step 2: Define a custom Dataset class\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, combined_features):\n",
    "        # Convert the combined features to a tensor\n",
    "        self.combined_features = torch.tensor(combined_features, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.combined_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.combined_features[idx]\n",
    "        return feature, feature  # For contrastive learning, use the same feature as both the input and the label\n",
    "\n",
    "# Step 3: Create DataLoader\n",
    "dataset = EmbeddingDataset(combined_features)\n",
    "train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Step 4: Define the custom model with an MLP\n",
    "class CustomEmbeddingModel(SentenceTransformer):\n",
    "    def __init__(self, combined_feature_dim):\n",
    "        # Initialize a SentenceTransformer with a custom MLP for feature processing\n",
    "        word_embedding_model = models.Transformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "        # Define the MLP with the appropriate input size (combined_feature_dim) and output embedding size (768)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(combined_feature_dim, 768),  # Adjust based on your input feature size\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(768, 768),  # Keep embedding dimension as 768 (for example)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.model(features)\n",
    "\n",
    "# Step 5: Initialize the model and loss function\n",
    "base_model = CustomEmbeddingModel(combined_feature_dim=600)  # 600 is the input size (as per combined_features)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=base_model)\n",
    "\n",
    "# Step 6: Train the model\n",
    "base_model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,  # Number of epochs\n",
    "    warmup_steps=100,  # Warmup steps\n",
    "    show_progress_bar=True  # Display progress bar\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ef8f8-f738-4e17-9aae-3ccc3a854194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)\n",
    "# [4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aba88d-acb9-412a-9f7a-fb399b461b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be064394-0d1f-4da7-ae15-7127dea4fb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split combined_features into separate components\n",
    "padded_posts = combined_features[:, :200]\n",
    "padded_bodies = combined_features[:, 200:400]\n",
    "padded_comments = combined_features[:, 400:]\n",
    "\n",
    "# Combine them into a list of tuples\n",
    "restructured_features = [\n",
    "    (padded_posts[i], padded_bodies[i], padded_comments[i])\n",
    "    for i in range(combined_features.shape[0])\n",
    "]\n",
    "\n",
    "# print(\"Restructured Features Count:\", len(restructured_features))\n",
    "# print(\"Example Restructured Feature (first sample):\", restructured_features[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2923c49-318f-43c3-a9ad-9a8bae8d493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Custom Dataset class to handle the embeddings\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, combined_features):\n",
    "        self.combined_features = torch.tensor(combined_features, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.combined_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the feature as both a positive and negative pair (for contrastive learning)\n",
    "        feature = self.combined_features[idx]\n",
    "        return feature, feature  # Dummy pair, modify as needed\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba03f51-6303-4327-98b2-8289ff7961a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and DataLoader\n",
    "dataset = EmbeddingDataset(combined_features)\n",
    "train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e945e17-bd46-472c-8628-213c3ed5603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple custom model that processes the combined embeddings\n",
    "class CustomEmbeddingModel(SentenceTransformer):\n",
    "    def __init__(self, combined_feature_dim):\n",
    "        # Initialize a SentenceTransformer with a custom MLP for feature processing\n",
    "        word_embedding_model = models.Transformer('paraphrase-MiniLM-L6-v2')\n",
    "        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "        # Replace the pooling layer with an MLP that works with the combined features\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(combined_feature_dim, 768),  # Adjust based on your input feature size\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(768, 768),  # Keep embedding dimension as 768 (for example)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.model(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717cfaa8-81df-49ca-9e96-2f4495f88d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47fb048-1c16-4666-972e-c8808e64d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Replace with the verified model identifier\n",
    "base_model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917428ee-0500-4cb2-9dcb-291b9528804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = models.Transformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c902e15d-1403-4652-ac7d-6427654a0fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import losses\n",
    "\n",
    "# Define the training loss\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=base_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d35f21b-5038-4adc-a8a3-9223cdce794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dense layer for dimensionality adjustment if necessary\n",
    "dense_layer = models.Dense(\n",
    "    in_features=restructured_features.shape[1],  # Input size matches combined_features\n",
    "    out_features=768,  # Match SentenceTransformer's output embedding dimension\n",
    "    activation_function=torch.nn.Tanh()\n",
    ")\n",
    "\n",
    "# Add Dense layer to the SentenceTransformer model\n",
    "base_model.add_module(\"dense_layer\", dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434211c1-22f9-423a-b430-90b056c65d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad96e1f2-96a9-4aff-b08c-69991942eeb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
