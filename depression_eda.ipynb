{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1091c84-047d-424c-a302-ea3eeafa4c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "796daed2-837a-4488-a230-450f69659d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # Show all column content\n",
    "pd.set_option('display.max_columns', None)  # Show all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "433303b0-8c76-4f05-8e6c-e046e470187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"datasets/reddit_posts_comments_Depression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74538615-1fea-4630-a99a-a73799c56bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv(\"datasets/reddit_posts_comments_Anxiety.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef0cb63-5902-4564-a901-3ef5a3841bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_post_1, df_body_1, df_comment_1 are from the first source\n",
    "# and df_post_2, df_body_2, df_comment_2 are from the second source\n",
    "\n",
    "# Create dataframes from each source\n",
    "df_1 = pd.DataFrame({\n",
    "    'Post Title': df_1['Post Title'],\n",
    "    'Post Body': df_1['Post Body'],\n",
    "    'Comment': df_1['Comment']\n",
    "})\n",
    "\n",
    "df_2 = pd.DataFrame({\n",
    "    'Post Title': df_2['Post Title'],\n",
    "    'Post Body': df_2['Post Body'],\n",
    "    'Comment': df_2['Comment']\n",
    "})\n",
    "\n",
    "# Concatenate the two datasets\n",
    "df = pd.concat([df_1, df_2], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "808c6221-c495-4095-bb0a-dd7eaf5fc825",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = df['Post Title']\n",
    "df_body = df['Post Body']\n",
    "df_comment = df['Comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be9a5b42-8f4b-4a57-b549-c6bffae2f923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_post = pd.DataFrame(df_post)\n",
    "df_post.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad764cce-140b-4137-9d41-43ff9ec805d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_body = pd.DataFrame(df_body)\n",
    "df_body.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f42d46a9-c105-4d7d-94ac-df5be6622814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_comment = pd.DataFrame(df_comment)\n",
    "df_comment.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "754b442d-f68c-4dd0-85b3-b13a8c7a6a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a934e33a-e386-4af9-8fe3-092fc7183aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all values are strings before applying strip() and lower()\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(lambda m: str(m).strip().lower() if isinstance(m, str) else \"\")\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(lambda m: str(m).strip().lower() if isinstance(m, str) else \"\")\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(lambda m: str(m).strip().lower() if isinstance(m, str) else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f71dfd9f-f7ad-4a8f-9a06-f5e3e964be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean and retain only words (strings)\n",
    "def clean_text(x):\n",
    "    if isinstance(x, str):  # Ensure it's a string\n",
    "        # Use regex to extract words, join them with spaces, and convert to lowercase\n",
    "        return \" \".join(re.findall(r'\\b\\w+', x))\n",
    "    return None  # Discard non-strings\n",
    "\n",
    "# Apply the cleaning function to the DataFrame columns\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(clean_text)\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(clean_text)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "657ef801-d17d-4a28-80c8-840d486e8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(lambda text: re.sub(r'\\s+', ' ',re.sub(r'_', ' ', re.sub(r'\\d+', '', text))).strip())\n",
    "\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(lambda text: re.sub(r'\\s+', ' ',re.sub(r'_', ' ', re.sub(r'\\d+', '', text))).strip())\n",
    "\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(lambda text: re.sub(r'\\s+', ' ',re.sub(r'_', ' ', re.sub(r'\\d+', '', text))).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71acb60a-b769-48f8-b225-fdac77b3c960",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10071783-6b8e-4cec-99de-d12c864b74ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea150e7a-f454-4f32-ac61-e2a41f3841a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e79aa2e-467c-478a-9668-7dae7365c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7f30fcd-ae18-4541-96ed-09c087ac5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(tokenizer.tokenize)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(tokenizer.tokenize)\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86d1d48e-666e-4735-a5bc-9fc2a89fa897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69aaba56-111e-42ab-9c12-623297c07f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\navne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a56b03c-fdf6-4c6b-9b27-143071dcba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a73e8f2-dfc5-4a76-abbb-22970489821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(message):\n",
    "    global stop_words\n",
    "    result = []\n",
    "    for word in str(message).split():  # Ensure the input is a string\n",
    "        if word.lower().strip() not in stop_words:\n",
    "            result.append(word)\n",
    "    return \" \".join(result)\n",
    "\n",
    "# Apply the function to each entry in the Post Body column\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)  # Convert lists to strings\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(remove_stopwords)\n",
    "\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)  # Convert lists to strings\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(remove_stopwords)\n",
    "\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)  # Convert lists to strings\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52c1556e-25fa-45ef-9897-6d49ad69a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "802a44ad-56ca-4975-9871-1959a2a5f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41c4e1e5-d00a-4e0d-8a6c-1e1da16c29ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba96bf25-5133-4a01-ae06-2dc19a3e2ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(tokenizer.tokenize)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(tokenizer.tokenize)\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "918e5453-f855-4ddc-a802-70bca527fe2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[understood, apologise, forget, future, break]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[biggest, problem, private, talks, may, may, reach, point, advice, one, needs, help, leave, chat, would, even, make, worse, public, chat, good, chance, others, may, jump, got, lost]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                 Comment\n",
       "0                                                                                                                                         [understood, apologise, forget, future, break]\n",
       "1                                                                                                                                                                              [removed]\n",
       "2  [biggest, problem, private, talks, may, may, reach, point, advice, one, needs, help, leave, chat, would, even, make, worse, public, chat, good, chance, others, may, jump, got, lost]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comment.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b945abba-455b-46ca-a8f3-df9636006e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment = df_comment[df_comment[\"Comment\"].apply(lambda x: x != [\"removed\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea429362-11c0-49bc-8462-cb7a65205281",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment = df_comment[df_comment[\"Comment\"].apply(lambda x: x != [\"deleted\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ab08ca1-fc70-4ad6-80a0-b8b21d4d7a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_comment.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "749e1816-66e1-4fce-a8c0-fa87a15bd0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = df_post.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc2aad79-0448-4abf-8bf4-b43322702266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_post = df_post.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "609c0e9c-59f6-42a2-b263-e2fba921053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8b7c5e6-9981-4b42-af44-5c57ee9b0def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"\n",
    "    Map POS tag to first character lemmatize() accepts.\n",
    "    Only adjectives (a) and verbs (v) are considered.\n",
    "    \"\"\"\n",
    "    tag = wordnet.synsets(word)\n",
    "    if tag:\n",
    "        pos = tag[0].pos()\n",
    "        if pos in {'a', 'v','n'}:  # Adjective or Verb\n",
    "            return pos\n",
    "    return None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Perform text cleaning including lemmatization, filtering, and deduplication.\n",
    "    Always keep certain whitelisted words.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Ensure input is a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # Tokenize text and remove non-alphabetic words\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "\n",
    "    # Define a whitelist of words to always keep\n",
    "    whitelist = {'depression', 'affect', 'psychiatrist', 'suicide', 'worse','sad','disturbing','mental','commit','broken','break'}\n",
    "\n",
    "    # Lemmatize and filter adjectives and verbs\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        pos = get_wordnet_pos(word)\n",
    "        lemma = lemmatizer.lemmatize(word.lower(), pos) if pos else word.lower()\n",
    "        if pos and lemma not in whitelist:  # Keep only adjectives and verbs unless in whitelist\n",
    "            lemmatized_words.append(lemma)\n",
    "\n",
    "    # Add whitelisted words explicitly\n",
    "    lemmatized_words.extend(word for word in words if word.lower() in whitelist)\n",
    "\n",
    "    # Remove duplicates and words differing by a single letter\n",
    "    unique_words = set()\n",
    "    final_words = []\n",
    "    for word in lemmatized_words:\n",
    "        if not any(len(word) == len(other) and sum(c1 != c2 for c1, c2 in zip(word, other)) == 1 for other in unique_words):\n",
    "            unique_words.add(word)\n",
    "            final_words.append(word)\n",
    "\n",
    "    return ' '.join(sorted(set(final_words)))\n",
    "\n",
    "# Convert all entries in 'Post Title' to strings and apply the cleaning function\n",
    "df_post['Post Title'] = df_post['Post Title'].astype(str)\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(clean_text)\n",
    "\n",
    "df_comment['Comment'] = df_comment['Comment'].astype(str)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(clean_text)\n",
    "\n",
    "df_body['Post Body'] = df_body['Post Body'].astype(str)\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(clean_text)\n",
    "\n",
    "# combining into single data frame\n",
    "df = pd.concat([df_post['Post Title'],df_body['Post Body'], df_comment['Comment']], axis=1)\n",
    "\n",
    "# Rename columns for clarity\n",
    "df.columns = ['Post Title','Post Body', 'Comment']\n",
    "\n",
    "# save the cleaned dataset to a new CSV file\n",
    "\n",
    "# df.to_csv('depression_cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb6b8375-6abf-4db4-8dda-197f6c6503a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# Combine the 'Post Title', 'Post Body', and 'Comment' into a single column\n",
    "df['combined_text'] = df['Post Title'] + \" \" + df['Post Body'] + \" \" + df['Comment']\n",
    "\n",
    "# Check for and handle missing values (NaN)\n",
    "df['combined_text'] = df['combined_text'].fillna('')  # Replace NaN with empty string\n",
    "\n",
    "# df['combined_text'] = df['combined_text'].apply(tokenizer.tokenize)\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=50000,ngram_range=(1,3))\n",
    "\n",
    "# Fit the vectorizer on the combined text from your DataFrame\n",
    "vectorizer.fit_transform(df['combined_text'])\n",
    "\n",
    "# Save the fitted vectorizer to a file\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b40e0938-c7a8-4786-bfdc-cc1a449a79db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_post_train,X_post_test = train_test_split(\n",
    "    df_post['Post Title'],test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d32df47-a6e6-43ee-a9ea-c445ad6546fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_body_train,X_body_test = train_test_split(\n",
    "    df_body['Post Body'],test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2af4a491-1e8c-4624-ba3b-32a24e1cd4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_comment_train,X_comment_test = train_test_split(\n",
    "    df_comment['Comment'],test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6558fe3a-138a-4c33-8e88-bd4616547a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of average vectors for posts: (569, 200)\n",
      "Shape of average vectors for bodies: (4111, 200)\n",
      "Shape of average vectors for comments: (4066, 200)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize all datasets\n",
    "tokenized_post_train = [word_tokenize(post.lower()) for post in X_post_train]\n",
    "tokenized_body_train = [word_tokenize(body.lower()) for body in X_body_train]\n",
    "tokenized_comment_train = [word_tokenize(comment.lower()) for comment in X_comment_train]\n",
    "\n",
    "# Combine all tokenized data for Word2Vec training\n",
    "combined_tokenized_data = tokenized_post_train + tokenized_body_train + tokenized_comment_train\n",
    "\n",
    "# Train Word2Vec model on the combined data\n",
    "model = Word2Vec(\n",
    "    sentences=combined_tokenized_data,\n",
    "    vector_size=200,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "model.save('word2vec_model')\n",
    "\n",
    "# Function to compute average Word2Vec vectors\n",
    "def avg_word2vec(sentence, model, vector_size):\n",
    "    word_vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Compute average vectors for each dataset\n",
    "avg_vectors_post = np.array([avg_word2vec(sentence, model, 200) for sentence in tokenized_post_train])\n",
    "avg_vectors_body = np.array([avg_word2vec(sentence, model, 200) for sentence in tokenized_body_train])\n",
    "avg_vectors_comment = np.array([avg_word2vec(sentence, model, 200) for sentence in tokenized_comment_train])\n",
    "\n",
    "# Print shapes of the resulting vectors\n",
    "print(\"Shape of average vectors for posts:\", avg_vectors_post.shape)\n",
    "print(\"Shape of average vectors for bodies:\", avg_vectors_body.shape)\n",
    "print(\"Shape of average vectors for comments:\", avg_vectors_comment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d054781-71e7-4a3e-83d4-e8296a226927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Combined Features: (4111, 600)\n"
     ]
    }
   ],
   "source": [
    "# combinig vectors\n",
    "import numpy as np\n",
    "\n",
    "# Determine the maximum number of samples\n",
    "max_samples = max(len(avg_vectors_post), len(avg_vectors_body), len(avg_vectors_comment))\n",
    "\n",
    "# Define a zero vector for padding\n",
    "zero_vector = np.zeros((200,))\n",
    "\n",
    "# Pad the smaller datasets with zero vectors\n",
    "padded_posts = np.vstack([\n",
    "    avg_vectors_post,\n",
    "    np.tile(zero_vector, (max_samples - len(avg_vectors_post), 1))\n",
    "])\n",
    "\n",
    "padded_bodies = np.vstack([\n",
    "    avg_vectors_body,\n",
    "    np.tile(zero_vector, (max_samples - len(avg_vectors_body), 1))\n",
    "])\n",
    "\n",
    "padded_comments = np.vstack([\n",
    "    avg_vectors_comment,\n",
    "    np.tile(zero_vector, (max_samples - len(avg_vectors_comment), 1))\n",
    "])\n",
    "\n",
    "# Combine the padded datasets into one\n",
    "combined_features = np.hstack([padded_posts, padded_bodies, padded_comments])\n",
    "\n",
    "print(\"Shape of Combined Features:\", combined_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40b51eca-f2b4-4863-8497-811d86041834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "Epoch 1/20, Loss: 24.1708\n",
      "Epoch 2/20, Loss: 24.1718\n",
      "Epoch 3/20, Loss: 24.1569\n",
      "Epoch 4/20, Loss: 24.1809\n",
      "Epoch 5/20, Loss: 24.1635\n",
      "Epoch 6/20, Loss: 24.1626\n",
      "Epoch 7/20, Loss: 24.1601\n",
      "Epoch 8/20, Loss: 24.1663\n",
      "Epoch 9/20, Loss: 24.1693\n",
      "Epoch 10/20, Loss: 24.1635\n",
      "Epoch 11/20, Loss: 24.1711\n",
      "Epoch 12/20, Loss: 24.1542\n",
      "Epoch 13/20, Loss: 24.1616\n",
      "Epoch 14/20, Loss: 24.1771\n",
      "Epoch 15/20, Loss: 24.1716\n",
      "Epoch 16/20, Loss: 24.1652\n",
      "Epoch 17/20, Loss: 24.1638\n",
      "Epoch 18/20, Loss: 24.1706\n",
      "Epoch 19/20, Loss: 24.1689\n",
      "Epoch 20/20, Loss: 24.1611\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define combined_features and labels\n",
    "# combined_features = np.random.rand(4111, 600)  # Replace with your actual data\n",
    "labels = np.random.randint(0, 3, 100)  # Example labels\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "features_tensor = torch.tensor(combined_features, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Adjust sizes to match\n",
    "if features_tensor.size(0) > labels_tensor.size(0):\n",
    "    # Pad labels tensor with random labels to match features\n",
    "    extra_labels = torch.randint(0, 3, (features_tensor.size(0) - labels_tensor.size(0),))\n",
    "    labels_tensor = torch.cat((labels_tensor, extra_labels), dim=0)\n",
    "elif features_tensor.size(0) < labels_tensor.size(0):\n",
    "    # Truncate labels tensor to match features\n",
    "    labels_tensor = labels_tensor[:features_tensor.size(0)]\n",
    "\n",
    "# Ensure sizes match after adjustment\n",
    "assert features_tensor.size(0) == labels_tensor.size(0), \"Features and labels must match in size.\"\n",
    "\n",
    "# Reshape the features tensor for CNN input\n",
    "height = 20\n",
    "width = features_tensor.size(1) // height\n",
    "print(width)\n",
    "features_tensor = features_tensor.view(-1, 1, height, width)  # Reshape to (batch, channel, height, width)\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=192, shuffle=True)\n",
    "\n",
    "class DepressionClassifierCNN(nn.Module):\n",
    "    def __init__(self, input_channels, height, width, num_classes):\n",
    "        super(DepressionClassifierCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)  # This will downsample by a factor of 2\n",
    "        \n",
    "        # Dynamically calculate flattened_dim based on desired size\n",
    "        self.fc1 = None  # Initializing here to be set later in forward\n",
    "\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        # print(f\"Shape after conv1: {x.shape}\")  # Debugging shape\n",
    "        x = self.pool(x)\n",
    "        # print(f\"Shape after pool1: {x.shape}\")  # Debugging shape\n",
    "        \n",
    "        x = self.relu(self.conv2(x))\n",
    "        # print(f\"Shape after conv2: {x.shape}\")  # Debugging shape\n",
    "        x = self.pool(x)\n",
    "        # print(f\"Shape after pool2: {x.shape}\")  # Debugging shape\n",
    "        \n",
    "        # Now, make sure that the flattened dimension becomes 192\n",
    "        # We want the final feature map size to be (batch_size, 32, 6, 6), since 32 * 6 * 6 = 192\n",
    "        x = self.pool(x)  # Additional pooling layer to reduce dimensions to 6x6\n",
    "        # print(f\"Shape after additional pooling: {x.shape}\")  # Debugging shape\n",
    "\n",
    "        flattened_dim = x.numel() // x.size(0)  # Flattened size excluding batch dimension\n",
    "        # print(f\"Flattened dimension: {flattened_dim}\")  # Debugging flattened dimension\n",
    "        \n",
    "        # Now we define fc1 with the correct flattened dimension\n",
    "        self.fc1 = nn.Linear(flattened_dim, 128)\n",
    "        \n",
    "        # Flatten the tensor and pass through fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = DepressionClassifierCNN(input_channels=1, height=height, width=width, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for features, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "    \n",
    "# Saving only the state dict (i.e., model weights)\n",
    "torch.save(model.state_dict(), \"depression_classifier_cnn.pth\")\n",
    "\n",
    "# save whole model\n",
    "torch.save(model, \"depression_classifier_full_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b1f25-5fe4-4233-b686-243f1e6a4bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
