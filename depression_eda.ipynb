{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1091c84-047d-424c-a302-ea3eeafa4c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "796daed2-837a-4488-a230-450f69659d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # Show all column content\n",
    "pd.set_option('display.max_columns', None)  # Show all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "433303b0-8c76-4f05-8e6c-e046e470187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"reddit_posts_comments_Depression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f82fe5ea-8d44-494f-b192-6eb406e4cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = df['Post Title']\n",
    "df_body = df['Post Body']\n",
    "df_comment = df['Comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be9a5b42-8f4b-4a57-b549-c6bffae2f923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_post = pd.DataFrame(df_post)\n",
    "df_post.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad764cce-140b-4137-9d41-43ff9ec805d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_body = pd.DataFrame(df_body)\n",
    "df_body.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f42d46a9-c105-4d7d-94ac-df5be6622814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_comment = pd.DataFrame(df_comment)\n",
    "df_comment.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "754b442d-f68c-4dd0-85b3-b13a8c7a6a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a934e33a-e386-4af9-8fe3-092fc7183aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(lambda m: m.strip().lower())\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(lambda m: m.strip().lower())\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(lambda m: m.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f71dfd9f-f7ad-4a8f-9a06-f5e3e964be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean and retain only words (strings)\n",
    "def clean_text(x):\n",
    "    if isinstance(x, str):  # Ensure it's a string\n",
    "        # Use regex to extract words, join them with spaces, and convert to lowercase\n",
    "        return \" \".join(re.findall(r'\\b\\w+', x))\n",
    "    return None  # Discard non-strings\n",
    "\n",
    "# Apply the cleaning function to the DataFrame columns\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(clean_text)\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(clean_text)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "657ef801-d17d-4a28-80c8-840d486e8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(lambda text: re.sub(r'\\s+', ' ',re.sub(r'_', ' ', re.sub(r'\\d+', '', text))).strip())\n",
    "\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(lambda text: re.sub(r'\\s+', ' ',re.sub(r'_', ' ', re.sub(r'\\d+', '', text))).strip())\n",
    "\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(lambda text: re.sub(r'\\s+', ' ',re.sub(r'_', ' ', re.sub(r'\\d+', '', text))).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71acb60a-b769-48f8-b225-fdac77b3c960",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10071783-6b8e-4cec-99de-d12c864b74ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea150e7a-f454-4f32-ac61-e2a41f3841a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e79aa2e-467c-478a-9668-7dae7365c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7f30fcd-ae18-4541-96ed-09c087ac5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(tokenizer.tokenize)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86d1d48e-666e-4735-a5bc-9fc2a89fa897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69aaba56-111e-42ab-9c12-623297c07f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\navne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a56b03c-fdf6-4c6b-9b27-143071dcba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a73e8f2-dfc5-4a76-abbb-22970489821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(message):\n",
    "    global stop_words\n",
    "    result = []\n",
    "    for word in str(message).split():  # Ensure the input is a string\n",
    "        if word.lower().strip() not in stop_words:\n",
    "            result.append(word)\n",
    "    return \" \".join(result)\n",
    "\n",
    "# Apply the function to each entry in the Post Body column\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)  # Convert lists to strings\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(remove_stopwords)\n",
    "\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)  # Convert lists to strings\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52c1556e-25fa-45ef-9897-6d49ad69a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "802a44ad-56ca-4975-9871-1959a2a5f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41c4e1e5-d00a-4e0d-8a6c-1e1da16c29ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba96bf25-5133-4a01-ae06-2dc19a3e2ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_post['Post Title'] = df_post['Post Title'].apply(tokenizer.tokenize)\n",
    "#df_comment['Comment'] = df_comment['Comment'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "918e5453-f855-4ddc-a802-70bca527fe2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>understood apologise forget future break</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>removed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>biggest problem private talks may may reach point advice one needs help leave chat would even make worse public chat good chance others may jump got lost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                     Comment\n",
       "0                                                                                                                   understood apologise forget future break\n",
       "1                                                                                                                                                    removed\n",
       "2  biggest problem private talks may may reach point advice one needs help leave chat would even make worse public chat good chance others may jump got lost"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comment.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b945abba-455b-46ca-a8f3-df9636006e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment = df_comment[df_comment[\"Comment\"].apply(lambda x: x != [\"removed\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ab08ca1-fc70-4ad6-80a0-b8b21d4d7a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_comment.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "749e1816-66e1-4fce-a8c0-fa87a15bd0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = df_post.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc2aad79-0448-4abf-8bf4-b43322702266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_post = df_post.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "609c0e9c-59f6-42a2-b263-e2fba921053a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\navne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7742bbd-557c-474d-995d-fd1ff6c8b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "# Initialize the spaCy model and lemmatizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to process each row\n",
    "def process_row(text):\n",
    "    if isinstance(text, str):  # If the row is a string\n",
    "        words = text.split()  # Split by spaces to get words\n",
    "    elif isinstance(text, list):  # If the row is a list\n",
    "        words = text\n",
    "    else:\n",
    "        return text  # Return as is for other types\n",
    "\n",
    "    # Process the text with spaCy to get POS tagging\n",
    "    doc = nlp(' '.join(words))  # Convert the list of words back to text for spaCy processing\n",
    "\n",
    "    seen = set()\n",
    "    lemmatized_words = []\n",
    "    \n",
    "    # Loop through the words and apply lemmatization based on POS tags\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['ADJ', 'VERB', 'NOUN']:  # Check if the word is an adjective or verb\n",
    "            lemma = token.lemma_.lower()  # Lemmatize and convert to lowercase\n",
    "            if lemma not in seen:  # Add only unique lemmas\n",
    "                seen.add(lemma)\n",
    "                lemmatized_words.append(lemma)\n",
    "    \n",
    "    return lemmatized_words if isinstance(text, list) else ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply the function to the 'Comment' and 'Post Title' columns\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(process_row)\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(process_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb9af21e-bdf5-4b22-9533-bf4be9432fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize and remove similar words\n",
    "def remove_similar_words(words):\n",
    "    # Set to store words and remove duplicates\n",
    "    lemmatized_words = set()\n",
    "    \n",
    "    # Apply lemmatization and store base form in set to avoid duplicates\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word.lower())  # Lemmatize and lowercase the word\n",
    "        lemmatized_words.add(lemma)\n",
    "    \n",
    "    # Return the unique lemmatized words\n",
    "    return list(lemmatized_words)\n",
    "\n",
    "# Apply the function to the 'Comment' column\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(remove_similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78be1734-23da-4279-9e5a-66c8b87d29ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>break least understand rule helper invite private contact first resort make new wiki explain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>regular check post information rule wikis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>psychiatrist make sign suicide contract</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     Post Title\n",
       "0  break least understand rule helper invite private contact first resort make new wiki explain\n",
       "1                                                     regular check post information rule wikis\n",
       "2                                                       psychiatrist make sign suicide contract"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_post.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c07d27-75d4-4470-8e65-9fce020a2f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003820d4-eb06-4c2d-970e-eb9e24d9c4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
