{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1091c84-047d-424c-a302-ea3eeafa4c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "796daed2-837a-4488-a230-450f69659d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # Show all column content\n",
    "pd.set_option('display.max_columns', None)  # Show all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "433303b0-8c76-4f05-8e6c-e046e470187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"reddit_posts_comments_Depression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f82fe5ea-8d44-494f-b192-6eb406e4cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = df['Post Title']\n",
    "df_body = df['Post Body']\n",
    "df_comment = df['Comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be9a5b42-8f4b-4a57-b549-c6bffae2f923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_post = pd.DataFrame(df_post)\n",
    "df_post.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad764cce-140b-4137-9d41-43ff9ec805d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_body = pd.DataFrame(df_body)\n",
    "df_body.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f42d46a9-c105-4d7d-94ac-df5be6622814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_comment = pd.DataFrame(df_comment)\n",
    "df_comment.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "754b442d-f68c-4dd0-85b3-b13a8c7a6a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a934e33a-e386-4af9-8fe3-092fc7183aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(lambda m: m.strip().lower())\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(lambda m: m.strip().lower())\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(lambda m: m.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f71dfd9f-f7ad-4a8f-9a06-f5e3e964be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean and retain only words (strings)\n",
    "def clean_text(x):\n",
    "    if isinstance(x, str):  # Ensure it's a string\n",
    "        # Use regex to extract words, join them with spaces, and convert to lowercase\n",
    "        return \" \".join(re.findall(r'\\b\\w+', x))\n",
    "    return None  # Discard non-strings\n",
    "\n",
    "# Apply the cleaning function to the DataFrame columns\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(clean_text)\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(clean_text)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "657ef801-d17d-4a28-80c8-840d486e8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(lambda text: re.sub(r'\\s+', ' ',re.sub(r'_', ' ', re.sub(r'\\d+', '', text))).strip())\n",
    "\n",
    "df_body['Post Body'] = df_body['Post Body'].apply(lambda text: re.sub(r'\\s+', ' ',re.sub(r'_', ' ', re.sub(r'\\d+', '', text))).strip())\n",
    "\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(lambda text: re.sub(r'\\s+', ' ',re.sub(r'_', ' ', re.sub(r'\\d+', '', text))).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71acb60a-b769-48f8-b225-fdac77b3c960",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10071783-6b8e-4cec-99de-d12c864b74ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea150e7a-f454-4f32-ac61-e2a41f3841a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e79aa2e-467c-478a-9668-7dae7365c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7f30fcd-ae18-4541-96ed-09c087ac5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(tokenizer.tokenize)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86d1d48e-666e-4735-a5bc-9fc2a89fa897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69aaba56-111e-42ab-9c12-623297c07f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\navne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a56b03c-fdf6-4c6b-9b27-143071dcba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a73e8f2-dfc5-4a76-abbb-22970489821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(message):\n",
    "    global stop_words\n",
    "    result = []\n",
    "    for word in str(message).split():  # Ensure the input is a string\n",
    "        if word.lower().strip() not in stop_words:\n",
    "            result.append(word)\n",
    "    return \" \".join(result)\n",
    "\n",
    "# Apply the function to each entry in the Post Body column\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)  # Convert lists to strings\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(remove_stopwords)\n",
    "\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)  # Convert lists to strings\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52c1556e-25fa-45ef-9897-6d49ad69a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "802a44ad-56ca-4975-9871-1959a2a5f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41c4e1e5-d00a-4e0d-8a6c-1e1da16c29ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba96bf25-5133-4a01-ae06-2dc19a3e2ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post['Post Title'] = df_post['Post Title'].apply(tokenizer.tokenize)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "918e5453-f855-4ddc-a802-70bca527fe2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[understood, apologise, forget, future, break]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[biggest, problem, private, talks, may, may, reach, point, advice, one, needs, help, leave, chat, would, even, make, worse, public, chat, good, chance, others, may, jump, got, lost]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                 Comment\n",
       "0                                                                                                                                         [understood, apologise, forget, future, break]\n",
       "1                                                                                                                                                                              [removed]\n",
       "2  [biggest, problem, private, talks, may, may, reach, point, advice, one, needs, help, leave, chat, would, even, make, worse, public, chat, good, chance, others, may, jump, got, lost]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comment.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b945abba-455b-46ca-a8f3-df9636006e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment = df_comment[df_comment[\"Comment\"].apply(lambda x: x != [\"removed\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea429362-11c0-49bc-8462-cb7a65205281",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment = df_comment[df_comment[\"Comment\"].apply(lambda x: x != [\"deleted\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ab08ca1-fc70-4ad6-80a0-b8b21d4d7a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_comment.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "749e1816-66e1-4fce-a8c0-fa87a15bd0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = df_post.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc2aad79-0448-4abf-8bf4-b43322702266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_post = df_post.reset_index(drop=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "609c0e9c-59f6-42a2-b263-e2fba921053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8b7c5e6-9981-4b42-af44-5c57ee9b0def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"\n",
    "    Map POS tag to first character lemmatize() accepts.\n",
    "    Only adjectives (a) and verbs (v) are considered.\n",
    "    \"\"\"\n",
    "    tag = wordnet.synsets(word)\n",
    "    if tag:\n",
    "        pos = tag[0].pos()\n",
    "        if pos in {'a', 'v'}:  # Adjective or Verb\n",
    "            return pos\n",
    "    return None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Perform text cleaning including lemmatization, filtering, and deduplication.\n",
    "    Always keep certain whitelisted words.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Ensure input is a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # Tokenize text and remove non-alphabetic words\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "\n",
    "    # Define a whitelist of words to always keep\n",
    "    whitelist = {'depression', 'affect', 'psychiatrist', 'suicide', 'worse','sad','disturbing','mental','commit','broken','break'}\n",
    "\n",
    "    # Lemmatize and filter adjectives and verbs\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        pos = get_wordnet_pos(word)\n",
    "        lemma = lemmatizer.lemmatize(word.lower(), pos) if pos else word.lower()\n",
    "        if pos and lemma not in whitelist:  # Keep only adjectives and verbs unless in whitelist\n",
    "            lemmatized_words.append(lemma)\n",
    "\n",
    "    # Add whitelisted words explicitly\n",
    "    lemmatized_words.extend(word for word in words if word.lower() in whitelist)\n",
    "\n",
    "    # Remove duplicates and words differing by a single letter\n",
    "    unique_words = set()\n",
    "    final_words = []\n",
    "    for word in lemmatized_words:\n",
    "        if not any(len(word) == len(other) and sum(c1 != c2 for c1, c2 in zip(word, other)) == 1 for other in unique_words):\n",
    "            unique_words.add(word)\n",
    "            final_words.append(word)\n",
    "\n",
    "    return ' '.join(sorted(set(final_words)))\n",
    "\n",
    "# Convert all entries in 'Post Title' to strings and apply the cleaning function\n",
    "df_post['Post Title'] = df_post['Post Title'].astype(str)\n",
    "df_post['Post Title'] = df_post['Post Title'].apply(clean_text)\n",
    "\n",
    "df_comment['Comment'] = df_comment['Comment'].astype(str)\n",
    "df_comment['Comment'] = df_comment['Comment'].apply(clean_text)\n",
    "\n",
    "# combining into single data frame\n",
    "# df = pd.concat([df_post['Post Title'], df_comment['Comment']], axis=1)\n",
    "\n",
    "# Rename columns for clarity\n",
    "# df.columns = ['Post Title', 'Comment']\n",
    "\n",
    "# save the cleaned dataset to a new CSV file\n",
    "\n",
    "# df.to_csv('cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2cfb4152-2627-49d8-9698-352eacc1abff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                              Post Title\n",
       "0    broken explain make new understand\n",
       "1                                      \n",
       "2             make psychiatrist suicide\n",
       "3                                      \n",
       "4                     affect depression\n",
       "..                                  ...\n",
       "301                                    \n",
       "302                                fail\n",
       "303                                pour\n",
       "304                                    \n",
       "305                                want\n",
       "\n",
       "[306 rows x 1 columns]>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_post.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c0b71-2c4e-40dc-98e2-402e36b27477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6784d11-9944-4004-b433-1ff79fb87ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
